<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Stock Price Scraper | Signs of Improvement</title><meta name=description content="Hey there! My name is Jared. I am a financial analyst living in Portland, OR. Here you can find my writings on all things ranging from economics, finance, writing, philosophy, and computers. I love reasoning about changes that can have marginal improvements on both the micro and macro scale."><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="Stock Price Scraper"><meta property="og:description" content="Understanding Pandas in Python: This was one of my first projects in python to understand data frames. Anyone interested in data analysis with Python needs to understand how to use, manipulate, and structure data in ways that are useful and required for your analysis. Thankfully, there are some incredibly smart developers in this world that understand the importance of this task, so they developed the Pandas package. On top of that, we can use the ever useful BeautifulSoup package, which allows us to make automated scripts to interact with HTML backed webpages."><meta property="og:type" content="article"><meta property="og:url" content="https://jaredtconnor.com/projects/stock-price-scaper/"><meta property="article:published_time" content="2017-08-30T00:00:00+00:00"><meta property="article:modified_time" content="2017-08-30T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Stock Price Scraper"><meta name=twitter:description content="Understanding Pandas in Python: This was one of my first projects in python to understand data frames. Anyone interested in data analysis with Python needs to understand how to use, manipulate, and structure data in ways that are useful and required for your analysis. Thankfully, there are some incredibly smart developers in this world that understand the importance of this task, so they developed the Pandas package. On top of that, we can use the ever useful BeautifulSoup package, which allows us to make automated scripts to interact with HTML backed webpages."><link rel=stylesheet href=https://jaredtconnor.com/css/style-white.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://jaredtconnor.com/images/favicon.ico></head><body class="max-width mx-auto px3 ltr"><div class="content index py4"><header id=header><a href=https://jaredtconnor.com><div id=logo style=background-image:url(https://jaredtconnor.com/images/logo.png)></div><div id=title><h1>Signs of Improvement</h1></div></a><div id=nav><ul><li class=icon><a href=#><i class="fas fa-bars fa-2x"></i></a></li><li><a href=/>Home</a></li><li><a href=/blog>Writings</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About</a></li></ul></div></header><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class=content itemprop=articleBody><h1 id=understanding-pandas-in-python>Understanding Pandas in Python:</h1><p>This was one of my first projects in python to understand data frames. Anyone interested in
data analysis with Python needs to understand how to use, manipulate, and structure data in
ways that are useful and required for your analysis. Thankfully, there are some <em>incredibly</em>
smart developers in this world that understand the importance of this task, so they developed
the <a href=https://pandas.pydata.org><strong>Pandas</strong></a> package. On top of that, we can use the ever useful <a href=https://www.crummy.com/software/BeautifulSoup/bs4/doc/><strong>BeautifulSoup</strong></a>
package, which allows us to make automated scripts to interact with HTML backed webpages.</p><p>The way Beautiful soup works is by making an HTML request to a specific web page, and
given the structure of the webpage, we can specify places where we want to <em>scrape</em> information.</p><p>First off, here are the dependencies:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> bs4 <span style=color:#f92672>as</span> bs
<span style=color:#f92672>import</span> datetime <span style=color:#f92672>as</span> dt
<span style=color:#f92672>import</span> os
<span style=color:#f92672>import</span> pandas <span style=color:#f92672>as</span> pd
<span style=color:#f92672>import</span> pandas_datareader.data <span style=color:#f92672>as</span> web
<span style=color:#f92672>from</span> pandas_datareader._utils <span style=color:#f92672>import</span> RemoteDataError
<span style=color:#f92672>import</span> pickle
<span style=color:#f92672>import</span> requests
</code></pre></div><p>This function below does just that: Obviously, we could write out a long list that contains all
of the ticker symbols for the S&P 500. But, there is probably structured data on some website
(<em>cough</em> Wikipedia <em>cough</em>) that we can use for the heavy lifting.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
<span style=color:#75715e>#Scrapes wikipedia S&amp;P 500 ticker table to create object</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>save_sp500_tickers</span>():
    resp <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;https://en.wikipedia.org/wiki/List_of_S%26P_500_companies&#34;</span>)
    soup <span style=color:#f92672>=</span> bs<span style=color:#f92672>.</span>BeautifulSoup(resp<span style=color:#f92672>.</span>text, <span style=color:#e6db74>&#34;html.parser&#34;</span>)
    table <span style=color:#f92672>=</span> soup<span style=color:#f92672>.</span>find(<span style=color:#e6db74>&#39;table&#39;</span>, {<span style=color:#e6db74>&#39;class&#39;</span>:<span style=color:#e6db74>&#39;wikitable sortable&#39;</span>})
    tickers <span style=color:#f92672>=</span> []

    <span style=color:#66d9ef>for</span> row <span style=color:#f92672>in</span> table<span style=color:#f92672>.</span>find_all(<span style=color:#e6db74>&#39;tr&#39;</span>)[<span style=color:#ae81ff>1</span>:]:
        ticker <span style=color:#f92672>=</span> row<span style=color:#f92672>.</span>find_all(<span style=color:#e6db74>&#39;td&#39;</span>)[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>text
        tickers<span style=color:#f92672>.</span>append(ticker)

    <span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#39;sp500_tickers.pickle&#39;</span>, <span style=color:#e6db74>&#39;wb&#39;</span>) <span style=color:#66d9ef>as</span> f:
        pickle<span style=color:#f92672>.</span>dump(tickers, f)

</code></pre></div><p>This gives us a great list for all 500 ticker symbols. If you aren&rsquo;t familiar with Pickles
in Python, they are essentially a write-able file that can store data in a more compressed
manner than, say, a csv file. More info on them can be found <a href=https://ianlondon.github.io/blog/pickling-basics/>here</a></p><p>Now for the fun stuff, we need to actually pull the pricing info for given period of time.
We will first pickle in our ticker list, then actually use the yahoo finance port via the
pandas data reader library to pull real time pricing information for us. This package was
maintained back when I began this project, but I believe it&rsquo;s been discontinued.
What can still be used is the <a href=https://pandas-datareader.readthedocs.io/en/latest/>data reader</a> package associated with pandas.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
<span style=color:#75715e>#Gathering all ADJ close data from Yahoo API for tickers created</span>
<span style=color:#75715e>#and storing in stock_data directory</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_data_from_yahoo</span>(reload_sp500<span style=color:#f92672>=</span>False):
    <span style=color:#66d9ef>if</span> reload_sp500:
        tickers <span style=color:#f92672>=</span> save_sp500_tickers()
    <span style=color:#66d9ef>else</span>:
        <span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;sp500_tickers.pickle&#34;</span>, <span style=color:#e6db74>&#34;rb&#34;</span>) <span style=color:#66d9ef>as</span> f:
            tickers <span style=color:#f92672>=</span> pickle<span style=color:#f92672>.</span>load(f)

    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(<span style=color:#e6db74>&#34;stock_data&#34;</span>):
        os<span style=color:#f92672>.</span>makedirs(<span style=color:#e6db74>&#34;stock_data&#34;</span>)


    start <span style=color:#f92672>=</span> dt<span style=color:#f92672>.</span>datetime(<span style=color:#ae81ff>2000</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>)
    end <span style=color:#f92672>=</span> dt<span style=color:#f92672>.</span>datetime(<span style=color:#ae81ff>2016</span>,<span style=color:#ae81ff>12</span>,<span style=color:#ae81ff>31</span>)

    <span style=color:#66d9ef>for</span> ticker <span style=color:#f92672>in</span> tickers:
        <span style=color:#66d9ef>try</span>:
            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(<span style=color:#e6db74>&#34;stock_data/{}&#34;</span><span style=color:#f92672>.</span>format(ticker)):
                df <span style=color:#f92672>=</span> web<span style=color:#f92672>.</span>DataReader(ticker, <span style=color:#e6db74>&#39;yahoo&#39;</span>, start, end)
                df<span style=color:#f92672>.</span>to_csv(<span style=color:#e6db74>&#34;stock_data/{}&#34;</span><span style=color:#f92672>.</span>format(ticker))
                <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Gathered data on {}&#34;</span><span style=color:#f92672>.</span>format(ticker))
            <span style=color:#66d9ef>else</span>:
                <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Already have data for {}&#34;</span><span style=color:#f92672>.</span>format(ticker))

        <span style=color:#66d9ef>except</span> RemoteDataError:
            <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Data Error&#34;</span>)
            <span style=color:#66d9ef>continue</span>
            
get_data_from_yahoo()

</code></pre></div><p>The main for loop here is the money maker. We&rsquo;re going to iteratively step through each
ticker, and using the pandas data reader, specify to pull the high price, low price, and
volume for the specified date range on a daily basis. For each of these, we&rsquo;re going to pop
this data out into a simple CSV file according to the ticker&rsquo;s name. Throw in some
additional try/except catches and this process could easily work for pulling stock price information.</p><h3 id=what-did-we-learn>What did we learn:</h3><p>While this was one of my first projects, and hindsight is 20/20, this started a rather deep and
continued dive into thinking about problems from a data perspective. While I learned about data
abstraction in my later computer science courses and am still learning about how to properly store
this data, thinking this way provides a simple heuristic to solve problems:</p><blockquote><p>Any problem can typically be solved with information, and information exists as data somewhere.</p></blockquote></div></article><footer id=footer><div class=footer-left>Copyright &copy; 2020 Jared C.</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/blog>Writings</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script><script src=/js/main.js></script></html>